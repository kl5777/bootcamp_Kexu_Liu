{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd9d785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo CSV created at data/instructor_dirty.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate demo CSV if not exists\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = 'data/instructor_dirty.csv'\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    df_demo = pd.DataFrame({\n",
    "        'numeric_col': [10, None, 40, 55, 70],\n",
    "        'category_col': ['A', 'B', 'A', 'B', 'C'],\n",
    "        'price': ['$100', '$200', '$150', None, '$250'],\n",
    "        'date_str': ['2025-08-01','2025-08-02',None,'2025-08-04','2025-08-05'],\n",
    "        'category': ['Electronics','Furniture','Toys','Clothing',None]\n",
    "    })\n",
    "    df_demo.to_csv(csv_path, index=False)\n",
    "    print(f\"Demo CSV created at {csv_path}\")\n",
    "else:\n",
    "    print(f\"CSV already exists at {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4b893",
   "metadata": {},
   "source": [
    "# Stage 6: Comprehensive Data Preprocessing\n",
    "- Missing data handling (MCAR/MAR/MNAR, visualization, fill/drop strategies)\n",
    "- Filtering and threshold-based row drops\n",
    "- Type corrections (currency, dates, categorical)\n",
    "- Scaling/normalization (MinMax, StandardScaler)\n",
    "- Reusable functions for reproducibility and workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7efeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc58bfa",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ec586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load instructor CSV\n",
    "df = pd.read_csv('data/instructor_dirty.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af9db0",
   "metadata": {},
   "source": [
    "## Inspect Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info and missing counts\n",
    "df.info()\n",
    "df.isna().sum()\n",
    "\n",
    "# Heatmap for missing data\n",
    "sns.heatmap(df.isnull(), cbar=False)\n",
    "plt.show()\n",
    "\n",
    "# missingno visualization\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2478c3",
   "metadata": {},
   "source": [
    "## Simulate MCAR / MAR / MNAR Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR: random missing\n",
    "df['MCAR_col'] = df['numeric_col'].mask(np.random.rand(len(df)) < 0.1)\n",
    "# MAR: missing depends on another column\n",
    "df['MAR_col'] = df['numeric_col'].mask(df['category_col']=='A')\n",
    "# MNAR: missing depends on its own value\n",
    "df['MNAR_col'] = df['numeric_col'].mask(df['numeric_col']>50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ef622",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing with median\n",
    "df['numeric_col'] = df['numeric_col'].fillna(df['numeric_col'].median())\n",
    "\n",
    "# Drop rows with missing MCAR\n",
    "df.dropna(subset=['MCAR_col'], inplace=True)\n",
    "\n",
    "# Alternative strategies\n",
    "df_fill_mean = df.fillna(df.mean(numeric_only=True))\n",
    "df_fill_median = df.fillna(df.median(numeric_only=True))\n",
    "df_fill_ffill = df.fillna(method='ffill')\n",
    "\n",
    "# Threshold-based row drop example\n",
    "df_drop_thresh = df.dropna(thresh=int(0.5*df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42349916",
   "metadata": {},
   "source": [
    "## Filtering Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['price'].str.replace('$','').astype(float) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3b134",
   "metadata": {},
   "source": [
    "## Type Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2aa72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currency string to numeric\n",
    "if 'price' in df.columns:\n",
    "    df['price'] = df['price'].str.replace('$','').astype(float)\n",
    "\n",
    "# Convert string to datetime\n",
    "if 'date_str' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date_str'], errors='coerce')\n",
    "\n",
    "# Standardize categorical column\n",
    "if 'category' in df.columns:\n",
    "    df['category'] = df['category'].str.lower().astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Column Type Demo ----\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# Demo DataFrame for column types\n",
    "df_types_demo = pd.DataFrame({\n",
    "    'all_integers': [1, 2, 3, 4],\n",
    "    'integers_with_nan': [1, 2, np.nan, 4],\n",
    "    'text_column': ['a', 'b', 'c', 'd'],\n",
    "    'mixed_column': [1, 'two', 3, 4]\n",
    "})\n",
    "\n",
    "print(\"Demo DataFrame:\")\n",
    "print(df_types_demo)\n",
    "\n",
    "# Show column types\n",
    "print(\"\\nColumn types:\")\n",
    "print(df_types_demo.dtypes)\n",
    "\n",
    "# Convert 'integers_with_nan' to nullable Int64\n",
    "df_types_demo['integers_with_nan'] = df_types_demo['integers_with_nan'].astype('Int64')\n",
    "print(\"\\nAfter converting 'integers_with_nan' to Int64:\")\n",
    "print(df_types_demo.dtypes)\n",
    "\n",
    "# Select numeric columns (generic)\n",
    "numeric_cols = df_types_demo.select_dtypes(include='number').columns\n",
    "print(\"\\nNumeric columns (generic):\", list(numeric_cols))\n",
    "\n",
    "# Select object columns (generic)\n",
    "object_cols = df_types_demo.select_dtypes(include='object').columns\n",
    "print(\"Object columns (generic):\", list(object_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615439a9",
   "metadata": {},
   "source": [
    "## Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fcdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scaling for numeric_col\n",
    "scaler = MinMaxScaler()\n",
    "df['numeric_scaled'] = scaler.fit_transform(df[['numeric_col']])\n",
    "\n",
    "# StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "df['numeric_standard'] = standardizer.fit_transform(df[['numeric_col']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1ed4e",
   "metadata": {},
   "source": [
    "## Reusable Functions for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fc76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_median(df, columns=None):\n",
    "    df_copy = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=np.number).columns\n",
    "    for col in columns:\n",
    "        df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    return df_copy\n",
    "\n",
    "def drop_missing(df, columns=None, threshold=None):\n",
    "    df_copy = df.copy()\n",
    "    if columns is not None:\n",
    "        return df_copy.dropna(subset=columns)\n",
    "    if threshold is not None:\n",
    "        return df_copy.dropna(thresh=int(threshold*df_copy.shape[1]))\n",
    "    return df_copy.dropna()\n",
    "\n",
    "def normalize_data(df, columns=None, method='minmax'):\n",
    "    df_copy = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df_copy.select_dtypes(include=np.number).columns\n",
    "    if method=='minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    df_copy[columns] = scaler.fit_transform(df_copy[columns])\n",
    "    return df_copy\n",
    "\n",
    "def correct_column_types(df):\n",
    "    df_copy = df.copy()\n",
    "    if 'price' in df_copy.columns:\n",
    "        df_copy['price'] = df_copy['price'].str.replace('$','').astype(float)\n",
    "    if 'date_str' in df_copy.columns:\n",
    "        df_copy['date'] = pd.to_datetime(df_copy['date_str'], errors='coerce')\n",
    "    if 'category' in df_copy.columns:\n",
    "        df_copy['category'] = df_copy['category'].str.lower().astype('category')\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79552497",
   "metadata": {},
   "source": [
    "## Validation & Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reusable functions\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned = fill_missing_median(df_cleaned)\n",
    "df_cleaned = drop_missing(df_cleaned, threshold=0.5)\n",
    "df_cleaned = normalize_data(df_cleaned)\n",
    "df_cleaned = correct_column_types(df_cleaned)\n",
    "\n",
    "# Inspect\n",
    "df_cleaned.info()\n",
    "df_cleaned.head()\n",
    "\n",
    "# Save\n",
    "df_cleaned.to_csv('data/processed/combined_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475c63d",
   "metadata": {},
   "source": [
    "## Preprocessing Assumptions\n",
    "\n",
    "When we clean and preprocess data, every choice encodes assumptions about the dataset. It's important to document these for reproducibility and stakeholder understanding.\n",
    "\n",
    "### 1. Missing Data Handling\n",
    "- Filling missing numeric values with median assumes the missingness is **MCAR or MAR** (not systematically biased).  \n",
    "- Forward/backward fill assumes **temporal continuity** in time series data.  \n",
    "- Dropping rows assumes the missing rows are **not critical** to analysis.  \n",
    "- Imputation affects averages, distributions, and model training.\n",
    "\n",
    "### 2. Understanding Missingness\n",
    "- MCAR: safe to drop or fill, assumes randomness.  \n",
    "- MAR: imputation using related features is valid.  \n",
    "- MNAR: missing depends on unobserved values; may require domain knowledge.  \n",
    "- Misidentifying missingness can bias results.\n",
    "\n",
    "### 3. Filtering / Data Cleaning\n",
    "- Removing negative or out-of-range values assumes they are **errors or invalid entries**.  \n",
    "- Dropping columns or rows with excessive missingness assumes those data are **non-essential**.  \n",
    "- Rare but valid events might be lost if thresholds are too strict.\n",
    "\n",
    "### 4. Scaling / Normalization\n",
    "- StandardScaler assumes features are roughly **normally distributed**.  \n",
    "- MinMaxScaler assumes min and max values are **representative**, not extreme outliers.  \n",
    "- Scaling changes interpretation of magnitudes; coefficients or distances may be affected.\n",
    "\n",
    "### 5. Column Type Corrections\n",
    "- Converting strings to numeric assumes there are **no hidden characters or formatting issues**.  \n",
    "- Parsing dates assumes a **consistent date format**.  \n",
    "- Categoricals assume a **finite, discrete set of values**.  \n",
    "- Wrong types can break computations or modeling.\n",
    "\n",
    "### 6. Reproducibility & Modularity\n",
    "- Using modular functions assumes **future datasets follow similar structure and patterns**.  \n",
    "- Documenting assumptions ensures that preprocessing is **transparent** and results are interpretable.\n",
    "\n",
    "> **Tip:** Always communicate these assumptions to stakeholders, so they understand the limitations and decisions made during preprocessing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
